\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wu2023interpretable,zhang2023skilful}
\citation{weron2014electricity}
\citation{lv2014traffic}
\newlabel{fig:Compare-Forecasting}{{1}{1}{Comparison between different forecasting paradigms. The inputs of forecasting with exogenous variables include multiple external variates $\{\mathbf {z}^{(1)},\cdots ,\mathbf {z}^{(C)}\}$, which serve as auxiliary information and are not required for forecasting}{figure.1}{}}
\newlabel{fig:Compare-Forecasting@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{olivares2023neural}
\citation{vaswani2017attention}
\citation{nie2022time}
\citation{liu2023itransformer}
\citation{lai2018modeling}
\citation{salinas2020deepar}
\citation{wu2023timesnet}
\citation{liu2022scinet}
\citation{oreshkin2019n}
\citation{zeng2023transformers}
\citation{devlin2018bert}
\citation{liu2021swin}
\citation{zhou2021informer,wu2021autoformer,liu2021pyraformer,zhou2022fedformer}
\citation{zhou2021informer}
\citation{wu2021autoformer}
\citation{liu2021pyraformer}
\citation{nie2022time}
\citation{zhang2022crossformer}
\citation{williams2001multivariate}
\citation{vagropoulos2016comparison}
\citation{lim2021temporal}
\citation{olivares2023neural}
\citation{das2023long}
\citation{liu2023itransformer}
\newlabel{fig:Related-Work}{{2}{3}{Illustration of different levels of representation for time series data, ranging from point-wise, patch-wise to series-wise}{figure.2}{}}
\newlabel{fig:Related-Work@cref}{{[figure][2][]2}{[1][3][]3}}
\citation{li2021align}
\newlabel{fig:structure}{{3}{4}{}{equation.3.1}{}}
\newlabel{fig:structure@cref}{{[section][3][]3}{[1][3][]4}}
\citation{lago2021forecasting}
\citation{liu2023itransformer}
\citation{nie2022time}
\citation{zhang2022crossformer}
\citation{liu2022non}
\citation{wu2021autoformer}
\citation{wu2023timesnet}
\citation{liu2022scinet}
\citation{li2023revisiting}
\citation{zeng2023transformers}
\citation{das2023long}
\citation{olivares2023neural}
\citation{liu2023itransformer}
\citation{li2023revisiting}
\citation{nie2022time}
\citation{zhang2022crossformer}
\citation{das2023long}
\citation{wu2023timesnet}
\citation{zeng2023transformers}
\citation{liu2022scinet}
\citation{liu2022non}
\citation{wu2021autoformer}
\citation{liu2023itransformer}
\citation{liu2023itransformer}
\citation{liu2023itransformer}
\citation{li2023revisiting}
\citation{nie2022time}
\citation{zhang2022crossformer}
\citation{das2023long}
\citation{wu2023timesnet}
\citation{zeng2023transformers}
\citation{liu2022scinet}
\citation{liu2022non}
\citation{wu2021autoformer}
\newlabel{tab:dataset}{{1}{5}{Dataset descriptions. Note that the number of endogenous variables is 1 for all datasets, corresponding to multiple exogenous variables. Detailed descriptions are listed in the Appendix \ref {tab:full-dataset}}{table.1}{}}
\newlabel{tab:dataset@cref}{{[table][1][]1}{[1][5][]5}}
\citation{dong2023simmtm}
\newlabel{tab:epf-forecast}{{2}{6}{Full results of the short-term forecasting task on EPF dataset. We follow the standard protocol in short-term electricity price forecasting, where the input length and predict length are set to 168 and 24 respectively for all baselines. Avg means the average results from all five datasets. The character ``.'' in the Transformers indicates the name of *former}{table.2}{}}
\newlabel{tab:epf-forecast@cref}{{[table][2][]2}{[1][5][]6}}
\newlabel{tab:multi-result}{{3}{6}{Full results of the long-term forecasting with exogenous variables. We compare extensive competitive models under different prediction lengths following the setting of iTransformer \cite {liu2023itransformer}. The input sequence length is set to 96 for all baselines. Results are averaged from all prediction lengths. The character ``.'' in the Transformers indicates the name of *former. ``-'' denotes out of memory (OOM) problem. The complete results are listed in the Appendix \ref {tab:full-log}}{table.3}{}}
\newlabel{tab:multi-result@cref}{{[table][3][]3}{[1][5][]6}}
\citation{kornblith2019similarity}
\citation{wu2023timesnet,dong2023simmtm}
\newlabel{fig:mask}{{4}{7}{Forecasting performance with the masked exogenous series on three EPF datasets, simulating the missing data scenario}{figure.4}{}}
\newlabel{fig:mask@cref}{{[figure][4][]4}{[1][6][]7}}
\newlabel{tab:abalation}{{4}{7}{Ablation Results. \emph {Ex.} and \emph {En.} are abbreviations for Exogenous variable and Endogenous variable. \emph {T} and \emph {V} denote temporal token and variate token respectively. Results are averaged from all prediction lengths. Full results are listed in the Appendix}{table.4}{}}
\newlabel{tab:abalation@cref}{{[table][4][]4}{[1][7][]7}}
\newlabel{fig:attn-vis}{{5}{7}{Visualization of learned attention map alongside the endogenous time series and the exogenous time series with highest and lowest attention scores}{figure.5}{}}
\newlabel{fig:attn-vis@cref}{{[figure][5][]5}{[1][7][]7}}
\citation{liu2023itransformer}
\citation{langley00}
\bibdata{ref}
\bibcite{das2023long}{{1}{2023}{{Das et~al.}}{{Das, Kong, Leach, Sen, and Yu}}}
\newlabel{fig:increase}{{6}{8}{Forecasting performance with the look-back length of exogenous variables varying from $\{96, 192, 336, 512, 720\}$ with a fixed look-back length of the endogenous variable to 96. Different styles of lines represent different prediction lengths. In most cases, the forecasting performance benefits from the increase of exogenous look-back length}{figure.6}{}}
\newlabel{fig:increase@cref}{{[figure][6][]6}{[1][7][]8}}
\newlabel{fig:cka}{{7}{8}{Comparison of representation CKA similarity between TimeXer and competitive baselines. \emph {iTrm-All} denotes the series representation of all variables learned by iTransformer, \emph {iTrm-En} is the learned series representation of the endogenous variable}{figure.7}{}}
\newlabel{fig:cka@cref}{{[figure][7][]7}{[1][7][]8}}
\newlabel{fig:multivariate-result}{{8}{8}{Performance Comparison between TimeXer with state-of-art models. Full results are listed in Appendix \ref {tab:full-log-multi}}{figure.8}{}}
\newlabel{fig:multivariate-result@cref}{{[figure][8][]8}{[1][8][]8}}
\bibcite{devlin2018bert}{{2}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dong2023simmtm}{{3}{2023}{{Dong et~al.}}{{Dong, Wu, Zhang, Zhang, Wang, and Long}}}
\bibcite{kingma2014adam}{{4}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{kornblith2019similarity}{{5}{2019}{{Kornblith et~al.}}{{Kornblith, Norouzi, Lee, and Hinton}}}
\bibcite{lago2021forecasting}{{6}{2021}{{Lago et~al.}}{{Lago, Marcjasz, De~Schutter, and Weron}}}
\bibcite{lai2018modeling}{{7}{2018}{{Lai et~al.}}{{Lai, Chang, Yang, and Liu}}}
\bibcite{li2021align}{{8}{2021}{{Li et~al.}}{{Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi}}}
\bibcite{li2019enhancing}{{9}{2019}{{Li et~al.}}{{Li, Jin, Xuan, Zhou, Chen, Wang, and Yan}}}
\bibcite{li2023revisiting}{{10}{2023}{{Li et~al.}}{{Li, Qi, Li, and Xu}}}
\bibcite{lim2021temporal}{{11}{2021}{{Lim et~al.}}{{Lim, Ar{\i }k, Loeff, and Pfister}}}
\bibcite{liu2022scinet}{{12}{2022{a}}{{Liu et~al.}}{{Liu, Zeng, Chen, Xu, Lai, Ma, and Xu}}}
\bibcite{liu2021pyraformer}{{13}{2021{a}}{{Liu et~al.}}{{Liu, Yu, Liao, Li, Lin, Liu, and Dustdar}}}
\bibcite{liu2022non}{{14}{2022{b}}{{Liu et~al.}}{{Liu, Wu, Wang, and Long}}}
\bibcite{liu2023itransformer}{{15}{2023}{{Liu et~al.}}{{Liu, Hu, Zhang, Wu, Wang, Ma, and Long}}}
\bibcite{liu2021swin}{{16}{2021{b}}{{Liu et~al.}}{{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}}}
\bibcite{lv2014traffic}{{17}{2014}{{Lv et~al.}}{{Lv, Duan, Kang, Li, and Wang}}}
\bibcite{nie2022time}{{18}{2022}{{Nie et~al.}}{{Nie, Nguyen, Sinthong, and Kalagnanam}}}
\bibcite{olivares2023neural}{{19}{2023}{{Olivares et~al.}}{{Olivares, Challu, Marcjasz, Weron, and Dubrawski}}}
\bibcite{oreshkin2019n}{{20}{2019}{{Oreshkin et~al.}}{{Oreshkin, Carpov, Chapados, and Bengio}}}
\bibcite{paszke2019pytorch}{{21}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.}}}
\bibcite{salinas2020deepar}{{22}{2020}{{Salinas et~al.}}{{Salinas, Flunkert, Gasthaus, and Januschowski}}}
\bibcite{vagropoulos2016comparison}{{23}{2016}{{Vagropoulos et~al.}}{{Vagropoulos, Chouliaras, Kardakos, Simoglou, and Bakirtzis}}}
\bibcite{vaswani2017attention}{{24}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{weron2014electricity}{{25}{2014}{{Weron}}{{}}}
\bibcite{williams2001multivariate}{{26}{2001}{{Williams}}{{}}}
\bibcite{wu2021autoformer}{{27}{2021}{{Wu et~al.}}{{Wu, Xu, Wang, and Long}}}
\bibcite{wu2023timesnet}{{28}{2023{a}}{{Wu et~al.}}{{Wu, Hu, Liu, Zhou, Wang, and Long}}}
\bibcite{wu2023interpretable}{{29}{2023{b}}{{Wu et~al.}}{{Wu, Zhou, Long, and Wang}}}
\bibcite{zeng2023transformers}{{30}{2023}{{Zeng et~al.}}{{Zeng, Chen, Zhang, and Xu}}}
\bibcite{zhang2022crossformer}{{31}{2022}{{Zhang \& Yan}}{{Zhang and Yan}}}
\bibcite{zhang2023skilful}{{32}{2023}{{Zhang et~al.}}{{Zhang, Long, Chen, Xing, Jin, Jordan, and Wang}}}
\bibcite{zhou2021informer}{{33}{2021}{{Zhou et~al.}}{{Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang}}}
\bibcite{zhou2022fedformer}{{34}{2022}{{Zhou et~al.}}{{Zhou, Ma, Wen, Wang, Sun, and Jin}}}
\bibstyle{icml2024}
\citation{li2019enhancing}
\citation{zhou2021informer}
\citation{zhou2021informer}
\citation{wu2023timesnet}
\citation{lago2021forecasting}
\citation{paszke2019pytorch}
\citation{kingma2014adam}
\citation{wu2023timesnet}
\newlabel{tab:full-dataset}{{5}{11}{Dataset descriptions. \emph {Ex.} and \emph {En.} are abbreviations for the Exogenous variable and Endogenous variable, respectively. The dataset size is organized in (Train, Validation, Test)}{table.5}{}}
\newlabel{tab:full-dataset@cref}{{[table][5][2147483647]5}{[1][11][]11}}
\newlabel{fig:enter-label}{{9}{12}{Model efficiency comparison on ECL and NP datasets}{figure.9}{}}
\newlabel{fig:enter-label@cref}{{[figure][9][2147483647]9}{[1][12][]12}}
\newlabel{fig:patch-len}{{10}{13}{Hyper-parameter sensitivity analysis of TimeXer on twelve real-world forecasting benchmarks}{figure.10}{}}
\newlabel{fig:patch-len@cref}{{[figure][10][2147483647]10}{[1][12][]13}}
\citation{kornblith2019similarity}
\citation{liu2023itransformer}
\citation{li2023revisiting}
\citation{nie2022time}
\citation{zhang2022crossformer}
\citation{das2023long}
\citation{wu2023timesnet}
\citation{zeng2023transformers}
\citation{liu2022scinet}
\citation{liu2022non}
\citation{wu2021autoformer}
\citation{liu2023itransformer}
\citation{li2023revisiting}
\citation{nie2022time}
\citation{zhang2022crossformer}
\citation{das2023long}
\citation{wu2023timesnet}
\citation{zeng2023transformers}
\citation{liu2022scinet}
\citation{liu2022non}
\citation{wu2021autoformer}
\newlabel{fig:showBE}{{11}{15}{Showcases of TimeXer in forecasting with exogenous variables from \textbf {BE} datasets. The two leftmost plots of the title "Generation" and "System Load" are the exogenous variables in the \textbf {BE} dataset. TimeXer outperforms all of its challengers by predicting all 4 injections in 24 prediction time points}{figure.11}{}}
\newlabel{fig:showBE@cref}{{[figure][11][2147483647]11}{[1][14][]15}}
\newlabel{fig:showDE}{{12}{15}{Showcases of TimeXer in forecasting with exogenous variables from \textbf {DE} datasets. The two leftmost plots of the title "System Load" and "Zonal COMED Load" are the exogenous variables in the \textbf {DE} dataset. TimeXer outperforms all of its challengers by predicting all 4 injections in 24 prediction time points}{figure.12}{}}
\newlabel{fig:showDE@cref}{{[figure][12][2147483647]12}{[1][14][]15}}
\newlabel{fig:showPJM}{{13}{16}{Showcases of TimeXer in forecasting with exogenous variables from \textbf {PJM} datasets. The two leftmost plots of the title "System Load" and "Zonal COMED Load" are the exogenous variables in the \textbf {PJM} dataset. TimeXer outperforms all of its challengers by predicting all 4 injections in 24 prediction time points}{figure.13}{}}
\newlabel{fig:showPJM@cref}{{[figure][13][2147483647]13}{[1][14][]16}}
\newlabel{fig:full-cka}{{14}{16}{Series Representation Analysis on three EPF datasets. \emph {iTrm-All} denotes the series representation of all variables learned by iTransformer, \emph {iTrm-En} is the learned series representation of the endogenous variable}{figure.14}{}}
\newlabel{fig:full-cka@cref}{{[figure][14][2147483647]14}{[1][14][]16}}
\newlabel{tab:full-log}{{7}{17}{Full results of the long-term forecasting with exogenous variables task}{table.7}{}}
\newlabel{tab:full-log@cref}{{[table][7][2147483647]7}{[1][14][]17}}
\newlabel{tab:full-log-multi}{{8}{18}{Full results of the long-term multivariate forecasting task}{table.8}{}}
\newlabel{tab:full-log-multi@cref}{{[table][8][2147483647]8}{[1][15][]18}}
\gdef \@abspage@last{18}
