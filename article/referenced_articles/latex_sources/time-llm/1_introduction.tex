Time series forecasting is a critical capability across many real-world dynamic systems~\citep{jin2023survey}, with applications ranging from demand planning \citep{leonard2001promotional} and inventory optimization \citep{li2022demand} to energy load forecasting \citep{liu2023sadi} and climate modeling \citep{schneider1974climate}. Each time series forecasting task typically requires extensive domain expertise and task-specific model designs. This stands in stark contrast to foundation language models like GPT-3 \citep{brown2020language}, GPT-4 \citep{openai2023gpt4}, Llama \citep{touvron2023llama}, \emph{inter alia}, which can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting.

Pre-trained foundation models, such as large language models (LLMs), have driven rapid progress in computer vision (CV) and natural language processing (NLP). While time series modeling has not benefited from the same significant breakthroughs, LLMs' impressive capabilities have inspired their application to time series forecasting~\citep{jin2023large}. Several desiderata exist for leveraging LLMs to advance forecasting techniques: \textbf{\textit{Generalizability.}} LLMs have demonstrated a remarkable capability for few-shot and zero-shot transfer learning \citep{brown2020language}. This suggests their potential for generalizable forecasting across domains without requiring per-task retraining from scratch. In contrast, current forecasting methods are often rigidly specialized by domain. \textbf{\textit{Data efficiency.}} By leveraging pre-trained knowledge, LLMs have shown the ability to perform new tasks with only a few examples. This data efficiency could enable forecasting for settings where historical data is limited. In contrast, current methods typically require abundant in-domain data. \textbf{\textit{Reasoning.}} LLMs exhibit sophisticated reasoning and pattern recognition capabilities \citep{mirchandani2023large,wang2023enhancing,chu2023leveraging}. Harnessing these skills could allow making highly precise forecasts by leveraging learned higher-level concepts.
Existing non-LLM methods are largely statistical without much innate reasoning. \textbf{\textit{Multimodal knowledge.}} As LLM architectures and training techniques improve, they gain more diverse knowledge across modalities like vision, speech, and text \citep{ma2023leveraging}. Tapping into this knowledge could enable synergistic forecasting that fuses different data types. Conventional tools lack ways to jointly leverage multiple knowledge bases. \textbf{\textit{Easy optimization.}} LLMs are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch. Optimizing existing forecasting models often requires significant architecture search and hyperparameter tuning \citep{zhou2023ptse}. In summary, LLMs offer a promising path to make time series forecasting more general, efficient, synergistic, and accessible compared to current specialized modeling paradigms. Thus, adapting these powerful models for time series data can unlock significant untapped potential.

The realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs' pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic. 

In this work, we propose \method, a reprogramming framework to adapt large language models for time series forecasting while keeping the backbone model intact. The core idea is to \textit{reprogram} the input time series into text prototype representations that are more naturally suited to language models' capabilities. To further augment the model's reasoning about time series concepts, we introduce \textit{Prompt-as-Prefix} (PaP), a novel idea in enriching the input time series with additional context and providing task instructions in the modality of natural language. This provides declarative guidance about desired transformations to apply to the reprogrammed input.
The output of the language model is then projected to generate time series forecasts. Our comprehensive evaluation demonstrates that large language models can act as effective few-shot and zero-shot time series learners when adopted through this reprogramming approach, outperforming specialized forecasting models. By leveraging LLMs' reasoning capability while keeping the models intact, our work points the way toward multimodal foundation models that can excel on both language and sequential data tasks. Our proposed reprogramming framework offers an extensible paradigm for imbuing large models with new capabilities beyond their original pre-training. Our main contributions in this work can be summarized as follows:

\vspace{-2mm}
\begin{itemize}
    \item We introduce a novel concept of \emph{reprogramming} large language models for time series forecasting without altering the pre-trained backbone model. In doing so, we show that forecasting can be cast as yet another ``language'' task that can be effectively tackled by an off-the-shelf LLM.
 
    \item We propose a new framework, \method, which encompasses reprogramming the input time series into text prototype representations that are more natural for the LLM, and augmenting the input context with declarative prompts (e.g., domain expert knowledge and task instructions) to guide LLM reasoning. Our technique points towards multimodal foundation models excelling in both language and time series.
 
    \item \method consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs' untapped potential for time series and perhaps other sequential data.
\end{itemize}
