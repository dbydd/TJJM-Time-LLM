@article{KingmaB14adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal = {International Conference on Learning Representations},
  year      = {2015}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{zhang2022cross,
  title={Cross reconstruction transformer for self-supervised time series representation learning},
  author={Zhang, Wenrui and Yang, Ling and Geng, Shijia and Hong, Shenda},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}
@article{zhang2022self,
  title={Self-supervised contrastive pre-training for time series via time-frequency consistency},
  author={Zhang, Xiang and Zhao, Ziyuan and Tsiligkaridis, Theodoros and Zitnik, Marinka},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3988--4003},
  year={2022}
}
@article{deldari2022beyond,
  title={Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data},
  author={Deldari, Shohreh and Xue, Hao and Saeed, Aaqib and He, Jiayuan and Smith, Daniel V and Salim, Flora D},
  journal={arXiv preprint arXiv:2206.02353},
  year={2022}
}

@inproceedings{zerveas2021transformer,
  title={A transformer-based framework for multivariate time series representation learning},
  author={Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2114--2124},
  year={2021}
}
@inproceedings{malhotra2017timenet,
  title={TimeNet: Pre-trained deep recurrent neural network for time series classification},
  author={Malhotra, Pankaj and TV, Vishnu and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
  booktitle={Proceedings of 25th European Symposium on Artificial Neural Networks},
  year={2017}
}
@inproceedings{fawaz2018transfer,
  title={Transfer learning for time series classification},
  author={Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  booktitle={IEEE International Conference on Big Data},
  pages={1367--1376},
  year={2018},
  organization={IEEE}
}

@inproceedings{yang2021voice2series,
  title={Voice2series: Reprogramming acoustic models for time series classification},
  author={Yang, Chao-Han Huck and Tsai, Yun-Yun and Chen, Pin-Yu},
  booktitle={International Conference on Machine Learning},
  pages={11808--11819},
  year={2021},
  organization={PMLR}
}

@article{chang2023llm4ts,
  title={LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs},
  author={Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu},
  journal={arXiv preprint arXiv:2308.08469},
  year={2023}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={11106--11115},
  year={2021}
}
@article{li2019enhancing,
  title={Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting},
  author={Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{smyl2020hybrid,
  title={A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting},
  author={Smyl, Slawek},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={75--85},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}
@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2018}
}
@book{box2015time,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}
@article{schneider1974climate,
  title={Climate modeling},
  author={Schneider, Stephen H and Dickinson, Robert E},
  journal={Reviews of Geophysics},
  volume={12},
  number={3},
  pages={447--493},
  year={1974},
  publisher={Wiley Online Library}
}
@article{ma2023leveraging,
  title={Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition},
  author={Ma, Ziyang and Wu, Wen and Zheng, Zhisheng and Guo, Yiwei and Chen, Qian and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2309.10294},
  year={2023}
}
@inproceedings{zhou2023ptse,
  title={pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting},
  author={Zhou, Yunyi and Chu, Zhixuan and Ruan, Yijia and Jin, Ge and Huang, Yuchen and Li, Sheng},
  booktitle={The 32nd International Joint Conference on Artificial Intelligence},
  year={2023}
}
@inproceedings{mirchandani2023large,
  title={Large Language Models as General Pattern Machines},
  author={Mirchandani, Suvir and Xia, Fei and Florence, Pete and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy and others},
  booktitle={Proceedings of the 7th Annual Conference on Robot Learning},
  year={2023}
}

@article{wang2023enhancing,
  title={Enhancing Recommender Systems with Large Language Model Reasoning Graphs},
  author={Wang, Yan and Chu, Zhixuan and Ouyang, Xin and Wang, Simeng and Hao, Hongyan and Shen, Yue and Gu, Jinjie and Xue, Siqiao and Zhang, James Y and Cui, Qing and others},
  journal={arXiv preprint arXiv:2308.10835},
  year={2023}
}

@article{chu2023leveraging,
  title={Leveraging Large Language Models for Pre-trained Recommender Systems},
  author={Chu, Zhixuan and Hao, Hongyan and Ouyang, Xin and Wang, Simeng and Wang, Yan and Shen, Yue and Gu, Jinjie and Cui, Qing and Li, Longfei and Xue, Siqiao and others},
  journal={arXiv preprint arXiv:2308.10837},
  year={2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hagan1987time,
  title={The time series approach to short term load forecasting},
  author={Hagan, Martin T and Behr, Suzanne M},
  journal={IEEE Transactions on Power Systems},
  volume={2},
  number={3},
  pages={785--791},
  year={1987},
  publisher={IEEE}
}

@article{li2022demand,
  title={From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization},
  author={Li, Na and Arnold, Donald M and Down, Douglas G and Barty, Rebecca and Blake, John and Chiang, Fei and Courtney, Tom and Waito, Marianne and Trifunov, Rick and Heddle, Nancy M},
  journal={Transfusion},
  volume={62},
  number={1},
  pages={87--99},
  year={2022},
  publisher={Wiley Online Library}
}

@article{leonard2001promotional,
  title={Promotional analysis and forecasting for demand planning: a practical time series approach},
  author={Leonard, Michael},
  journal={with exhibits},
  volume={1},
  year={2001},
  publisher={Citeseer}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{chen2022model,
  title={Model reprogramming: Resource-efficient cross-domain machine learning},
  author={Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2202.10629},
  year={2022}
}

@inproceedings{nie2022time,
title={A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers},
author={Yuqi Nie and Nam H Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
booktitle={International Conference on Learning Representations},
year={2023}
}

@article{huang2023lorahub,
  title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}

@inproceedings{kim2021reversible,
  title={Reversible instance normalization for accurate time-series forecasting against distribution shift},
  author={Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{misra2023reprogramming,
  title={Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets},
  author={Misra, Diganta and Goyal, Agam and Runwal, Bharat and Chen, Pin Yu},
  journal={arXiv preprint arXiv:2308.14969},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{yin2023survey,
  title={A Survey on Multimodal Large Language Models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@article{zhang2023self,
  title={Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects},
  author={Zhang, Kexin and Wen, Qingsong and Zhang, Chaoli and Cai, Rongyao and Jin, Ming and Liu, Yong and Zhang, James and Liang, Yuxuan and Pang, Guansong and Song, Dongjin and others},
  journal={arXiv preprint arXiv:2306.10125},
  year={2023}
}

@article{xue2022prompt,
  title={Prompt-Based Time Series Forecasting: A New Task and Dataset},
  author={Xue, Hao and Salim, Flora D},
  journal={arXiv preprint arXiv:2210.08964},
  year={2022}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@article{zhou2023one,
  title={One Fits All: Power General Time Series Analysis by Pretrained LM},
  author={Zhou, Tian and Niu, Peisong and Wang, Xue and Sun, Liang and Jin, Rong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}


@inproceedings{wu2022timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{makridakis2018m4,
  title={The M4 Competition: Results, findings, conclusion and way forward},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={International Journal of Forecasting},
  volume={34},
  number={4},
  pages={802--808},
  year={2018},
  publisher={Elsevier}
}

@article{liu2023large,
  title={Large Language Models are Few-Shot Health Learners},
  author={Liu, Xin and McDuff, Daniel and Kovacs, Geza and Galatzer-Levy, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  journal={arXiv preprint arXiv:2305.15525},
  year={2023}
}
 
@inproceedings{wen2023transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2023}
}

@article{woo2022etsformer,
  title={Etsformer: Exponential smoothing transformers for time-series forecasting},
  author={Woo, Gerald and Liu, Chenghao and Sahoo, Doyen and Kumar, Akshat and Hoi, Steven},
  journal={arXiv preprint arXiv:2202.01381},
  year={2022}
}

@article{liu2022non,
  title={Non-stationary transformers: Exploring the stationarity in time series forecasting},
  author={Liu, Yong and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9881--9893},
  year={2022}
}

@inproceedings{zhou2022fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International Conference on Machine Learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@inproceedings{liu2023sadi,
  title={SADI: A Self-Adaptive Decomposed Interpretable Framework for Electric Load Forecasting Under Extreme Events},
  author={Liu, Hengbo and Ma, Ziqing and Yang, Linxiao and Zhou, Tian and Xia, Rui and Wang, Yi and Wen, Qingsong and Sun, Liang},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
  year={2023},
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{zhang2022less,
  title={Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures},
  author={Zhang, Tianping and Zhang, Yizhuo and Cao, Wei and Bian, Jiang and Yi, Xiaohan and Zheng, Shun and Li, Jian},
  journal={arXiv preprint arXiv:2207.01186},
  year={2022}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={11121--11128},
  year={2023}
}

@inproceedings{challu2023nhits,
  title={NHITS: neural hierarchical interpolation for time series forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={6989--6997},
  year={2023}
}

@inproceedings{oreshkin2019n,
  title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
  author={Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{dong2023simmtm,
  title={SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling},
  author={Dong, Jiaxiang and Wu, Haixu and Zhang, Haoran and Zhang, Li and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{kojima2205large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@inproceedings{tang2022domain,
  title={Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities},
  author={Tang, Yihong and Qu, Ao and Chow, Andy HF and Lam, William HK and Wong, SC and Ma, Wei},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={1905--1915},
  year={2022}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{challu2022nhits,
  title={N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Garza, Federico and Mergenthaler, Max and Dubrawski, Artur},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2023}
}

@article{gruver2023large,
  title={Large Language Models Are Zero-Shot Time Series Forecasters},
  author={Gruver, Nate and Finzi, Marc Anton and Qiu, Shikai and Wilson, Andrew Gordon},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{makridakis2000m3,
  title={The M3-Competition: results, conclusions and implications},
  author={Makridakis, Spyros and Hibon, Michele},
  journal={International journal of forecasting},
  volume={16},
  number={4},
  pages={451--476},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{vinod2020reprogramming,
  title={Reprogramming Language Models for Molecular Representation Learning},
  author={Vinod, Ria and Chen, Pin-Yu and Das, Payel},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2020}
}

@inproceedings{melnyk2023reprogramming,
  title={Reprogramming Pretrained Language Models for Antibody Sequence Infilling},
  author={Melnyk, Igor and Chenthamarakshan, Vijil and Chen, Pin-Yu and Das, Payel and Dhurandhar, Amit and Padhi, Inkit and Das, Devleena},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{herzen2022darts,
  title={Darts: User-friendly modern machine learning for time series},
  author={Herzen, Julien and Lassig, Francesco and Piazzetta, Samuele Giuliano and Neuer, Thomas and Tafti, Leo and Raille, Guillaume and Van Pottelbergh, Tomas and Pasieka, Marek and Skrodzki, Andrzej and Huguenin, Nicolas and others},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5442--5447},
  year={2022},
  publisher={JMLRORG}
}

@article{jin2023large,
  title={Large models for time series and spatio-temporal data: A survey and outlook},
  author={Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and others},
  journal={arXiv preprint arXiv:2310.10196},
  year={2023}
}

@article{jin2023survey,
  title={A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection},
  author={Jin, Ming and Koh, Huan Yee and Wen, Qingsong and Zambon, Daniele and Alippi, Cesare and Webb, Geoffrey I and King, Irwin and Pan, Shirui},
  journal={arXiv preprint arXiv:2307.03759},
  year={2023}
}