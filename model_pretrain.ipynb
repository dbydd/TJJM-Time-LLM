{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from time_llm.data_provider_pretrain.data_factory import data_provider\n",
    "from time_llm.models import Autoformer, DLinear\n",
    "import model_predefines\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from time_llm.utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.environ['CURL_CA_BUNDLE'] = ''\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Time-LLM')\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "# basic config\n",
    "args.task_name = 'long_term_forecast' # 'task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]'\n",
    "args.is_training = 1 # 'status'\n",
    "args.model_id = 'test' # 'model id'\n",
    "args.model_comment = 'none' # 'prefix when saving test results'\n",
    "args.model = 'Autoformer' # 'model name, options: [Autoformer, DLinear]'\n",
    "args.seed = 2021 # 'random seed'\n",
    "\n",
    "# data loader\n",
    "args.data_pretrain = 'ETTm1' # 'dataset type'\n",
    "args.data = 'ETTm1' # 'dataset type'\n",
    "args.root_path = 'ETTh1' # 'data file'\n",
    "args.data_path_pretrain = 'ETTh1' # 'data file'\n",
    "args.features = 'M' # 'forecasting task, options:[M, S, MS]; ''M:multivariate predict multivariate, S: univariate predict univariate, ''MS:multivariate predict univariate'\n",
    "args.target = 'OT' # 'target feature in S or MS task'\n",
    "args.loader = 'modal' # 'dataset type'\n",
    "args.freq = 'h' # 'freq for time features encoding, ''options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], ''you can also use more detailed freq like 15min or 3h'\n",
    "args.checkpoints = 96 # 'input sequence length'\n",
    "args.label_len = 48 # 'start token length'\n",
    "args.pred_len = 96 # 'prediction sequence length'\n",
    "args.seasonal_patterns = 'Monthly' # 'subset for M4'\n",
    "\n",
    "# model define\n",
    "args.enc_in = 7 # 'encoder input size'\n",
    "args.dec_in = 7 # 'decoder input size'\n",
    "args.c_out = 7 # 'output size'\n",
    "args.d_model = 16 # 'dimension of model'\n",
    "args.n_heads = 8 # 'num of heads'\n",
    "args.e_layers = 2 # 'num of encoder layers'\n",
    "args.d_layers = 1 # 'num of decoder layers'\n",
    "args.d_ff = 32 # 'dimension of fcn'\n",
    "args.moving_avg = 25 # 'window size of moving average'\n",
    "args.factor = 1 # 'attn factor'\n",
    "args.dropout = 0 # 'dropout'\n",
    "args.embed = 'timeF' # 'time features encoding, options:[timeF, fixed, learned]'\n",
    "args.activation = 'gelu' # 'activation'\n",
    "args.output_attention = 16 # 'patch length'\n",
    "args.stride = 8 # 'stride'\n",
    "args.prompt_domain = 0 # ''\n",
    "args.llm_model = 'LLAMA' # 'LLM model' # LLAMA\n",
    "args.llm_dim = '4096' # 'LLM model dimension'# LLama7b:4096; GPT2-small:768; BERT-base:768\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10 # 'data loader num workers'\n",
    "args.itr = 1 # 'experiments times'\n",
    "args.train_epochs = 10 # 'train epochs'\n",
    "args.align_epochs = 10 # 'alignment epochs'\n",
    "args.batch_size = 32 # 'batch size of train input data'\n",
    "args.eval_batch_size = 8 # 'batch size of model evaluation'\n",
    "args.patience = 5 # 'early stopping patience'\n",
    "args.learning_rate = 0 # 'optimizer learning rate'\n",
    "args.des = 'test' # 'exp description'\n",
    "args.loss = 'MSE' # 'loss function'\n",
    "args.lradj = 'type1' # 'adjust learning rate'\n",
    "args.pct_start = 0 # 'pct_start'\n",
    "args.use_amp = False # use automatic mixed precision training\n",
    "args.llm_layers = False \n",
    "args.percent = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "# deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "# accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    train_data, train_loader = data_provider(args, args.data_pretrain, args.data_path_pretrain, True, 'train')\n",
    "    vali_data, vali_loader = data_provider(args, args.data_pretrain, args.data_path_pretrain, True, 'val')\n",
    "    test_data, test_loader = data_provider(args, args.data, args.data_path, False, 'test')\n",
    "\n",
    "    model = model_predefines.Model(args).float()\n",
    "\n",
    "    path = os.path.join(args.checkpoints,setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    args.content = load_content(args)\n",
    "    if not os.path.exists(path) and accelerator.is_local_main_process:\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(accelerator=accelerator, patience=args.patience)\n",
    "\n",
    "    trained_parameters = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad is True:\n",
    "            trained_parameters.append(p)\n",
    "\n",
    "    model_optim = optim.Adam(trained_parameters, lr=args.learning_rate)\n",
    "\n",
    "    if args.lradj == 'COS':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=args.pct_start,\n",
    "                                            epochs=args.train_epochs,\n",
    "                                            max_lr=args.learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_metric = nn.L1Loss()\n",
    "\n",
    "    train_loader, vali_loader, test_loader, model, model_optim, scheduler = accelerator.prepare(\n",
    "        train_loader, vali_loader, test_loader, model, model_optim, scheduler)\n",
    "\n",
    "    if args.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            batch_x = batch_x.float().to(accelerator.device)\n",
    "            batch_y = batch_y.float().to(accelerator.device)\n",
    "            batch_x_mark = batch_x_mark.float().to(accelerator.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(accelerator.device)\n",
    "\n",
    "            # decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(\n",
    "                accelerator.device)\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(\n",
    "                accelerator.device)\n",
    "\n",
    "            # encoder - decoder\n",
    "            if args.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    if args.output_attention:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                    else:\n",
    "                        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                    f_dim = -1 if args.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "                    batch_y = batch_y[:, -args.pred_len:, f_dim:].to(accelerator.device)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    train_loss.append(loss.item())\n",
    "            else:\n",
    "                if args.output_attention:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                else:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                f_dim = -1 if args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                accelerator.print(\n",
    "                    \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "                accelerator.print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "\n",
    "            if args.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                accelerator.backward(loss)\n",
    "                model_optim.step()\n",
    "\n",
    "            if args.lradj == 'TST':\n",
    "                adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, args, printout=False)\n",
    "                scheduler.step()\n",
    "\n",
    "        accelerator.print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)\n",
    "        test_loss, test_mae_loss = vali(args, accelerator, model, test_data, test_loader, criterion, mae_metric)\n",
    "        accelerator.print(\n",
    "            \"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f} MAE Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_loss, vali_loss, test_loss, test_mae_loss))\n",
    "\n",
    "        early_stopping(vali_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            accelerator.print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if args.lradj != 'TST':\n",
    "            if args.lradj == 'COS':\n",
    "                scheduler.step()\n",
    "                accelerator.print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "            else:\n",
    "                if epoch == 0:\n",
    "                    args.learning_rate = model_optim.param_groups[0]['lr']\n",
    "                    accelerator.print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "                adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, args, printout=True)\n",
    "\n",
    "        else:\n",
    "            accelerator.print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_local_main_process:\n",
    "    path = './checkpoints'  # unique checkpoint saving path\n",
    "    del_files(path)  # delete checkpoint files\n",
    "    accelerator.print('success delete checkpoints')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
