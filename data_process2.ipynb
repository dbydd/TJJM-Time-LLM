{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'datasets\\\\datasets_data\\\\collected\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取与预处理\n",
    "xlsx = []\n",
    "csv = []\n",
    "\n",
    "#ignored: ecodata, air_condition\n",
    "\n",
    "def read(path,date_key='时间',date_format = None,operations=None,dropsubset=None):\n",
    "    sheet = pd.read_csv(path,index_col= date_key,parse_dates=[date_key],date_format=date_format)\n",
    "    sheet.rename(columns={date_key:'date'},inplace=True)\n",
    "    sheet.dropna(subset=dropsubset)\n",
    "    if operations is not None:\n",
    "        sheet = operations(sheet)\n",
    "    sheet = sheet.resample('YE').mean()\n",
    "    return sheet\n",
    "\n",
    "# xlsx.append[([pd.read_excel(os.path.join(path,'carbon_emission.xlsx'))],'emission vector')]\n",
    "\n",
    "# csv.append(read(os.path.join(path,'air_pollution_events.csv'),operations=lambda x: x.filter(regex='大气污染事故次数:年')))\n",
    "csv.append(read(os.path.join(path,'citylize.csv')))\n",
    "csv.append(read(os.path.join(path,'comsume.csv'),'时间?',operations=lambda x : x.filter(regex='支出')))\n",
    "csv.append(read(os.path.join(path,'controllable_income.csv'),'时间'))\n",
    "csv.append(read(os.path.join(path,'energy_investment.csv'),operations= lambda x: x.filter(regex='固定资产投资额(不含农户):能源工业:上海市:年')))\n",
    "csv.append(read(os.path.join(path,'energy_structure.csv'),'时间?',operations=lambda x: x.filter(['能源生产总量构成:水电、核电、风电:年?','一次能源生产量:年?'])))\n",
    "# csv.append(read(os.path.join(path,'medic_insurance.csv'),operations=lambda sheet: sheet.resample('YE').mean())) # 数据太少，影响整体质量，不使用\n",
    "csv.append(read(os.path.join(path,'thick_cause_death.csv'),'时间?',operations= lambda x: x.filter(regex='(农村呼吸系统疾病粗死亡率_)|(城市呼吸系统疾病粗死亡率_)')))\n",
    "csv.append(read(os.path.join(path,'work_time.csv'),operations=lambda sheet: sheet.resample('YE').mean()))\n",
    "csv.append(read(os.path.join(path,'goverment_investment.csv'),'时间'))\n",
    "csv.append(read(r'D:\\huge_program_project\\TJJM-Time-LLM\\datasets\\datasets_data\\collected\\processed\\gdp\\上海市_gdp_yearly_sum.csv',date_key='年份',operations=lambda x: x.rename(columns={'总量（亿元）':'gdp'})))\n",
    "csv.append(read(r'D:\\huge_program_project\\TJJM-Time-LLM\\datasets\\datasets_data\\collected\\processed\\co2\\上海_emission.csv',date_key='year',operations=lambda x:x.rename(columns={'emission':'co2_emission'})))\n",
    "csv.append(read(r'D:\\huge_program_project\\TJJM-Time-LLM\\datasets\\datasets_data\\collected\\processed\\aqi\\上海市_yearly_aircondition.csv',date_key='年份',operations=lambda x:x.rename(columns={'AQI':'当年平均空气质量指数'})))\n",
    "\n",
    "\n",
    "csvs = pd.concat(csv,axis=1)\n",
    "csvs = csvs.loc['2010-12-31':'2021-12-31']\n",
    "\n",
    "rem = csvs.columns.difference(['date'])\n",
    "\n",
    "for col in rem:\n",
    "    csvs[col] = csvs[col].interpolate(method='spline', order=1)\n",
    "    csvs[col] = csvs[col].dropna()\n",
    "    \n",
    "\n",
    "csvs.rename(columns={\n",
    "    # '大气污染事故次数:年': '大气污染事故次数',\n",
    "    '乡村人口数:年': '乡村人口数',\n",
    "    '城镇人口数:年': '城镇人口数',\n",
    "    '城乡居民比例': '城乡人口比',\n",
    "    '农村家庭平均每人年消费性支出:年?': '农村人均消费支出',\n",
    "    '城镇居民人均消费性支出:年?': '城镇人均消费支出',\n",
    "    '居民可支配收入': '可支配收入',\n",
    "    '能源生产总量构成:水电、核电、风电:年?': '清洁能源占比',\n",
    "    '一次能源生产量:年?': '一次能源生产量', \n",
    "    '城市呼吸系统疾病粗死亡率_当期值_年?': '城市呼吸系统疾病死亡率',\n",
    "    '农村呼吸系统疾病粗死亡率_当期值_年?': '农村呼吸系统疾病死亡率',\n",
    "    '就业人员周平均工作时间:当期值:月': '周均工作时间',\n",
    "    'gdp': 'GDP',\n",
    "    'co2_emission': 'CO2排放量',\n",
    "    '当年平均空气质量指数': '空气质量指数',\n",
    "    '一般公共预算支出:年':'政府经费支出',\n",
    "    '一般公共预算支出:节能环保支出:污染防治:年':'政府污染防治经费支出'\n",
    "}, inplace=True)\n",
    "\n",
    "#计算额外指标\n",
    "csvs['环境治理支出占比'] = csvs['政府污染防治经费支出'] / csvs['政府经费支出'] * 100\n",
    "# 保留两位小数\n",
    "csvs['环境治理支出占比'] = csvs['环境治理支出占比'].round(2)\n",
    "\n",
    "\n",
    "csvs.to_csv(os.path.join(path,'processed','summarize.csv'),index_label='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#成本效益计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: (12, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['datasets\\\\\\\\datasets_data\\\\\\\\collected\\\\\\\\processed\\\\scalar.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据标准化\n",
    "import joblib\n",
    "# 读取summarize.csv\n",
    "df = pd.read_csv(r'datasets\\datasets_data\\collected\\processed\\summarize.csv',index_col='date',parse_dates=['date'])\n",
    "# 将'date'列转换为datetime类型\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 检查数据质量\n",
    "print(\"数据形状:\", df.shape)\n",
    "# print(\"数据类型:\\n\", df.dtypes)\n",
    "# print(\"缺失值检查:\\n\", df.isnull().sum())\n",
    "\n",
    "# 假设整理后的DataFrame变量名为df\n",
    "# 选择需要标准化的列\n",
    "columns = [ '乡村人口数', '城镇人口数', '农村人均消费支出', \n",
    "                    '城镇人均消费支出', '可支配收入', '一次能源生产量',\n",
    "                    '城市呼吸系统疾病死亡率', '农村呼吸系统疾病死亡率', '周均工作时间', \n",
    "                    'GDP', 'CO2排放量', '空气质量指数']\n",
    "\n",
    "# 初始化StandardScaler\n",
    "scaler = StandardScaler().fit(df[columns])\n",
    "\n",
    "# 对选定的列进行标准化\n",
    "df[columns] = scaler.transform(df[columns])\n",
    "\n",
    "# print(\"预处理后的数据:\\n\", df.head())\n",
    "df.to_csv(os.path.join(path,'processed',\"data_preprocessed.csv\"))  # 保存预处理后的数据\n",
    "joblib.dump(scaler, os.path.join(path,'processed',\"scalar.pkl\")) #保存缩放因子方便后续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据分析\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = ['SimHei']  # 使用中文字体\n",
    "\n",
    "# 读取预处理后的数据\n",
    "df = pd.read_csv(os.path.join(path,'processed',\"data_preprocessed.csv\"), index_col='date', parse_dates=['date'])\n",
    "\n",
    "# Granger因果检验\n",
    "maxlag = 2  # 设置最大时滞\n",
    "test_results = pd.DataFrame(np.zeros((len(columns), len(columns))),\n",
    "                            index=columns, columns=columns)\n",
    "for cause in columns:\n",
    "    for effect in columns:\n",
    "        if cause != effect:\n",
    "            test = grangercausalitytests(list(df[[cause, effect]].dropna().values), maxlag=maxlag, verbose=False)\n",
    "            p_values = [test[i+1][0]['ssr_ftest'][1] for i in range(maxlag)]\n",
    "            min_p_value = np.min(p_values)\n",
    "            test_results.loc[cause, effect] = min_p_value\n",
    "\n",
    "# 交叉相关分析\n",
    "lags = range(1, 11)  # 设置时滞范围\n",
    "cross_corr = pd.DataFrame(index=columns, columns=columns)\n",
    "for cause in columns:\n",
    "    for effect in columns:\n",
    "        if cause != effect:\n",
    "            cross_corr.loc[cause, effect] = df[cause].shift(1).corr(df[effect])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 20))\n",
    "sns.heatmap(test_results, ax=ax1, cmap='YlGnBu', annot=False,fmt='.3f')\n",
    "ax1.set_title('Granger Causality')\n",
    "sns.heatmap(cross_corr.astype(float), ax=ax2, cmap='YlOrRd', annot=False,fmt='.3f')\n",
    "ax2.set_title('Cross Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sample covariance matrix is not PD. It may indicate that data is bad. Also, it arises often when polychoric/polyserial correlations are used. semopy now will run nearPD subroutines.\n",
      "WARNING:root:Sample covariance matrix is not PD. It may indicate that data is bad. Also, it arises often when polychoric/polyserial correlations are used. semopy now will run nearPD subroutines.\n",
      "WARNING:root:Fisher Information Matrix is not PD.Moore-Penrose inverse will be used instead of Cholesky decomposition. See 10.1109/TSP.2012.2208105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit result:\n",
      " Name of objective: MLW\n",
      "Optimization method: SLSQP\n",
      "Optimization successful.\n",
      "Optimization terminated successfully\n",
      "Objective value: 602.289\n",
      "Number of iterations: 2\n",
      "Params: -0.999 -0.047 -0.994 -0.994 0.831 -0.694 0.227 -0.149 -0.229 0.776 10816.537 1.934 8812020.223 0.055 0.051 8813152.981 0.103 0.089 -0.002 0.001 -0.000 -0.005 67.750 -0.001 -62.283\n",
      "                     Value\n",
      "DoF             128.000000\n",
      "DoF Baseline    152.000000\n",
      "chi2           7227.472296\n",
      "chi2 p-value      0.000000\n",
      "chi2 Baseline  6398.866016\n",
      "CFI              -0.136485\n",
      "GFI              -0.129493\n",
      "AGFI             -0.341273\n",
      "NFI              -0.129493\n",
      "TLI              -0.349576\n",
      "RMSEA             2.245493\n",
      "AIC           -1154.578716\n",
      "BIC           -1142.456050\n",
      "LogLik          602.289358\n"
     ]
    }
   ],
   "source": [
    "#模型拟合\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import semopy\n",
    "from semopy import Model\n",
    "import os\n",
    "\n",
    "# 读取数据\n",
    "# data = pd.read_csv('data_preprocessed.csv', index_col='date', parse_dates=True)\n",
    "data = pd.read_csv(os.path.join(path, 'processed', 'data_preprocessed.csv'), index_col='date', parse_dates=True)\n",
    "\n",
    "model = \"\"\"\n",
    "# 潜变量\n",
    "      社会经济地位 =~ 乡村人口数 + 城镇人口数 + 城乡人口比 + 农村人均消费支出 + 城镇人均消费支出 + 可支配收入 + GDP\n",
    "      环境 =~ 清洁能源占比 + 一次能源生产量 + CO2排放量 + 空气质量指数\n",
    "      健康 =~ 城市呼吸系统疾病死亡率 + 农村呼吸系统疾病死亡率\n",
    "      工作状况 =~ 周均工作时间\n",
    "      政策 =~ 政府经费支出 + 政府污染防治经费支出 + 环境治理支出占比\n",
    "    \n",
    "      # 观测变量\n",
    "      乡村人口数 ~~ NA*乡村人口数\n",
    "      城镇人口数 ~~ NA*城镇人口数  \n",
    "      城乡人口比 ~~ NA*城乡人口比\n",
    "      农村人均消费支出 ~~ NA*农村人均消费支出\n",
    "      城镇人均消费支出 ~~ NA*城镇人均消费支出\n",
    "      可支配收入 ~~ NA*可支配收入\n",
    "      GDP ~~ NA*GDP\n",
    "      清洁能源占比 ~~ NA*清洁能源占比\n",
    "      一次能源生产量 ~~ NA*一次能源生产量\n",
    "      CO2排放量 ~~ NA*CO2排放量\n",
    "      空气质量指数 ~~ NA*空气质量指数\n",
    "      城市呼吸系统疾病死亡率 ~~ NA*城市呼吸系统疾病死亡率\n",
    "      农村呼吸系统疾病死亡率 ~~ NA*农村呼吸系统疾病死亡率\n",
    "      周均工作时间 ~~ NA*周均工作时间\n",
    "      政府经费支出 ~~ NA*政府经费支出\n",
    "      政府污染防治经费支出 ~~ NA*政府污染防治经费支出\n",
    "      环境治理支出占比 ~~ NA*环境治理支出占比\n",
    "    \n",
    "      # 路径关系  \n",
    "      健康 ~ 环境 + 社会经济地位 + 工作状况\n",
    "      环境 ~ 社会经济地位 + 政策\n",
    "      工作状况 ~ 社会经济地位\n",
    "      社会经济地位 ~ 政策\n",
    "\"\"\"\n",
    "\n",
    "# 定义模型\n",
    "model = Model(model)\n",
    "\n",
    "# 拟合模型\n",
    "print(f'fit result:\\n {model.fit(data)}')\n",
    "\n",
    "# 输出结果\n",
    "# print(model.inspect())\n",
    "\n",
    "print(semopy.calc_stats(model).T)\n",
    "g = semopy.semplot(model, os.path.join(\"article\",\"figures\",\"sem_model.png\"),show=True)\n",
    "\n",
    "# # 计算成本效益\n",
    "# def cost_benefit(policy_cost, health_benefit, discount_rate=0.05, time_horizon=20):\n",
    "#     net_benefit = 0\n",
    "#     for t in range(time_horizon):\n",
    "#         net_benefit += (health_benefit - policy_cost) / (1 + discount_rate)**t\n",
    "#     return net_benefit\n",
    "\n",
    "# policy_cost = 1000  # 假设的政策成本\n",
    "# health_benefit = model.predict(data, \"Health\") - model.predict(data, \"Health\")\n",
    "# health_benefit = health_benefit.mean() * 10000  # 假设每个单位对应1万元的健康效益\n",
    "\n",
    "# net_benefit = cost_benefit(policy_cost, health_benefit)\n",
    "# print(f\"净效益: {net_benefit}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
